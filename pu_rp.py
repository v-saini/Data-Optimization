# -*- coding: utf-8 -*-
"""PU_RP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AwVST7symOiktLXVbTlcZoUtFLxzGVgE
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries
# %reset -f
import pandas as pd
import random
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.offsetbox import AnchoredText
import matplotlib.colors as colors
from mpl_toolkits import mplot3d
from math import sqrt
import warnings

from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder,LabelEncoder


from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor,BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.cross_decomposition import PLSRegression
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import IsolationForest
from sklearn.metrics  import f1_score
from xgboost import XGBRegressor

import tensorflow as tf
import keras
from keras import layers
from keras.models import Sequential
from keras.layers import Dense
from keras.models import load_model
from keras.wrappers.scikit_learn import KerasClassifier

import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (10,10)
warnings.filterwarnings("ignore")

def defining_model(x):
    if x == 'mlr':
      model = LinearRegression()
    elif x=='adboost':
      model = AdaBoostRegressor()
    elif x=='xtratree':
      model = ExtraTreesRegressor()
    elif x=='bagging':
      model = BaggingRegressor()
    elif x=='pls':
      model = PLSRegression()
    elif x=='rndmfrst':
      model = RandomForestRegressor()
    elif x=='knn':
      model = KNeighborsRegressor()
    elif x=='svr':
      model = SVR()
    else:
      print("wrong selection")
    return model

# Getting data from CSV file
DF_data = pd.read_csv('dataset.csv')
temp_DF = DF_data.iloc[:,:5]
display(DF_data)

# Removing Unwanted columns
DF_data=DF_data.drop(['Name of the Nucleophile','Nucleophile ID Number','sN','Solvent'],axis=1)
DF_data['Type Of Nucleophile'].replace({'H-Nucleophile':0,'C-Nucleophile':1,'N-Nucleophile':2,'O-Nucleophile':3,'S-Nucleophile':4},inplace=True)
DF_data

Y = DF_data['N']# Target
DF_data.drop('N',axis=1,inplace=True) # Input variable

# Baseline Model
model = RandomForestRegressor()
scores = cross_val_score(model,DF_data,Y,cv = KFold(n_splits=10,shuffle = True))
print(scores)
print("Mean Score",np.mean(scores))

trainX,testX,trainY,testY = train_test_split(DF_data,Y,test_size= 0.1)

# Let's build Baseling Model
model = RandomForestRegressor()
model.fit(trainX,trainY)
model.score(testX,testY)

"""# Genetic Algorithm"""

from re import I
#defining various steps required for the genetic algorithm
def initilization_of_population(size,n_data_points): # size = size of population ( randome )
    population = []
    for i in range(size):
        chromosome = np.ones(n_data_points,dtype=np.bool)
        chromosome[:int(0.0001*n_data_points)]=False
        np.random.shuffle(chromosome)
        population.append(chromosome)
    return population

def Calculate_Fitness_Sample(population):
    scores = []
    for chromosome in population:
        
        X_train, Y_train = trainX[chromosome], trainY[chromosome]
        rf = RandomForestRegressor()
        rf.fit(X_train,Y_train)
        prediction = rf.predict(testX)
        score = r2_score(testY,prediction)
        scores.append(score)
    scores, population = np.array(scores), np.array(population) 
    inds = np.argsort(scores)
    return list(scores[inds][::-1]), list(population[inds,:][::-1])

def selection(pop_after_fit,n_parents):
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen

def crossover(pop_after_sel):
    population_nextgen=pop_after_sel
    for i in range(len(pop_after_sel)):
        child=pop_after_sel[i]
        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]
        population_nextgen.append(child)
    return population_nextgen

def mutation(pop_after_cross,mutation_rate):
    population_nextgen = []
    for i in range(0,len(pop_after_cross)):
        chromosome = pop_after_cross[i]
        for j in range(len(chromosome)):
            if random.random() < mutation_rate:
                chromosome[j]= not chromosome[j]
        population_nextgen.append(chromosome)
    return population_nextgen

def GA_iForest_RF_Sample (size,n_data_points,n_parents,mutation_rate,n_gen,X_train,
                                   X_test, y_train, y_test):
    best_chromo= []
    best_score= []
    population_nextgen=initilization_of_population(size,n_data_points)
    for i in range(n_gen):  
        scores, pop_after_fit = Calculate_Fitness_Sample(population_nextgen)
        pop_after_sel = selection(pop_after_fit,n_parents)
        pop_after_cross = crossover(pop_after_sel)
        population_nextgen = mutation(pop_after_cross,mutation_rate)
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])
        print(i)
        
    return best_chromo,best_score

chromo,score = GA_iForest_RF_Sample(size=4 ,n_data_points=len(trainX),n_parents=2,mutation_rate=0.01, ## size > n_parents
                     n_gen=10,X_train=trainX,X_test=testX,y_train=trainY,y_test=testY)

score

# new_training_data_aft_sampling 
x_train_sampling,y_train_sampling = trainX[chromo[np.argmax(score)]],trainY[chromo[np.argmax(score)]]
len(x_train_sampling)

# Let's build model with new dataset
model = RandomForestRegressor()
model.fit(x_train_sampling,y_train_sampling)
model.score(testX,testY)

#defining various steps required for the genetic algorithm
def initilization_of_population(size,n_feat): # size = size of population ( random )
    population = []
    for i in range(size):
        chromosome = np.ones(n_feat,dtype=np.bool)
        chromosome[:int(0.2*n_feat)]=False
        np.random.shuffle(chromosome)
        population.append(chromosome)
    return population

def Calculate_Fitness_Sample(population):
    scores = []
    for chromosome in population:
        X_train, Y_train = x_train_sampling.iloc[:,chromosome], y_train_sampling
        rf = RandomForestRegressor()
        rf.fit(X_train,Y_train)
        prediction = rf.predict(testX.iloc[:,chromosome])
        score = r2_score(testY,prediction)
        scores.append(score)
    scores, population = np.array(scores), np.array(population) 
    inds = np.argsort(scores)
    return list(scores[inds][::-1]), list(population[inds,:][::-1])

def selection(pop_after_fit,n_parents):
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen

def crossover(pop_after_sel):
    population_nextgen=pop_after_sel
    for i in range(len(pop_after_sel)):
        child=pop_after_sel[i]
        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]
        population_nextgen.append(child)
    return population_nextgen

def mutation(pop_after_cross,mutation_rate):
    population_nextgen = []
    for i in range(0,len(pop_after_cross)):
        chromosome = pop_after_cross[i]
        for j in range(len(chromosome)):
            if random.random() < mutation_rate:
                chromosome[j]= not chromosome[j]
        population_nextgen.append(chromosome)
    #print(population_nextgen)
    return population_nextgen

def GA_iForest_RF_Sample (size,n_feat,n_parents,mutation_rate,n_gen,X_train,
                                   X_test, y_train, y_test):
    best_chromo= []
    best_score= []
    population_nextgen=initilization_of_population(size,n_feat)
    for i in range(n_gen):  
        scores, pop_after_fit = Calculate_Fitness_Sample(population_nextgen)
        pop_after_sel = selection(pop_after_fit,n_parents)
        pop_after_cross = crossover(pop_after_sel)
        population_nextgen = mutation(pop_after_cross,mutation_rate)
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])
        print(i)
        
    return best_chromo,best_score

chromo,score = GA_iForest_RF_Sample(size=10 ,n_feat=len(trainX.columns),n_parents=5,mutation_rate=0.10, ## size > n_parents
                     n_gen=10,X_train=x_train_sampling,X_test=testX,y_train=y_train_sampling,y_test=testY)

score

# new_training_data_aft_sampling 
new_x_train,new_y_train,new_x_test = x_train_sampling.iloc[:,chromo[np.argmax(score)]],y_train_sampling,testX.iloc[:,chromo[np.argmax(score)]]

print("best_features")
new_x_train

# Let's build model with new dataset
model = RandomForestRegressor()
model.fit(new_x_train,new_y_train)
model.score(new_x_test,testY)

# Let's find best parameters.....
param_grid = {  'bootstrap': [True], 
              'max_depth': [7,10,12,15,None],
              'max_features': ['auto', 'log2'], 
              'n_estimators': [100,150,120],
              'min_samples_leaf': [1,2],
              'min_samples_split': [2,1]}

rfr = RandomForestRegressor(random_state = 1)

g_search = GridSearchCV(estimator = rfr, param_grid = param_grid, cv =10, n_jobs = 1, verbose = 0, return_train_score=True)
g_search.fit(new_x_train, new_y_train);

print(g_search.best_params_)

print(g_search.score(testX.iloc[:,chromo[np.argmax(score)]], testY))

print(g_search.best_params_)

# Random forest with default parameters on the new dataset
model = RandomForestRegressor ()
model.fit(new_x_train,new_y_train)
model.score(new_x_test,testY)

# Let's build model with new dataset
model = RandomForestRegressor (bootstrap =  True, 
                               max_depth =  12, 
                               max_features =  'auto', 
                               min_samples_leaf =  1, 
                               min_samples_split =  2, 
                               n_estimators =  150)
model.fit(new_x_train,new_y_train)
model.score(new_x_test,testY)

model.get_params()

model.predict(new_x_test)

sns.set(style='whitegrid')
sns.set_context("paper", font_scale=2)

plt.rcParams["figure.figsize"] = (10,10)
plt.rcParams['savefig.dpi'] = 400
plt.rcParams["savefig.format"] = 'tiff'
f, ax = plt.subplots(1,1)
actual=testY
predicted=model.predict(new_x_test)
sns.regplot(actual,predicted, color='midnightblue')

print("Mean absolute error (MAE):      %f" % mean_absolute_error(actual,predicted))
print("Mean squared error (MSE):       %f" % mean_squared_error(actual,predicted))
print("Root mean squared error (RMSE): %f" % sqrt(mean_squared_error(actual,predicted)))
print("R square (R^2):                 %f" % r2_score(actual,predicted))


plt.xlabel('Acutal')
plt.ylabel('Predicted')
plt.suptitle("Actual Vs Predicted")
anchored_text = AnchoredText("R\u00b2 Score  "+str(round(r2_score(actual,predicted),2)), loc=2,prop=dict(size=20))
ax.add_artist(anchored_text)
plt.savefig(str(model)[1:6])
plt.tight_layout()
plt.show()

predictions = pd.DataFrame()
temp_DF[['Type Of Nucleophile','Solvent']]

predictions = temp_DF.iloc[new_x_test.index]
predictions.drop(['Nucleophile ID Number'],axis=1,inplace=True)

predictions['predictions'] =  np.round(predicted,2)

# Saving the file
predictions.to_csv('Predictions.csv',index=False)

predictions

